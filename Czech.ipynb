{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM7zyIc-wnh5",
        "outputId": "3bcd11ef-dfdb-4a07-83a5-8c9f4c1975ee"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'ces.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-Czech.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-Czech.pkl\n",
            "[hi] => [ahoj]\n",
            "[run] => [bez]\n",
            "[run] => [utikej]\n",
            "[who] => [kdo]\n",
            "[wow] => [pani]\n",
            "[fire] => [hori]\n",
            "[fire] => [ohen]\n",
            "[hello] => [ahoj]\n",
            "[hurry] => [pospes si]\n",
            "[hurry] => [delej]\n",
            "[i see] => [aha]\n",
            "[i see] => [ach tak]\n",
            "[i see] => [vidim]\n",
            "[attack] => [utok]\n",
            "[attack] => [do utoku]\n",
            "[cheers] => [na zdravi]\n",
            "[eat it] => [snez to]\n",
            "[get up] => [vstavej]\n",
            "[he ran] => [bezel]\n",
            "[he ran] => [utikal]\n",
            "[i know] => [vim]\n",
            "[i know] => [ja vim]\n",
            "[really] => [opravdu]\n",
            "[really] => [skutecne]\n",
            "[really] => [vazne]\n",
            "[thanks] => [diky]\n",
            "[try it] => [zkus to]\n",
            "[come in] => [vstupte]\n",
            "[drop it] => [pust to]\n",
            "[i agree] => [souhlasim]\n",
            "[i stink] => [smrdim]\n",
            "[i stink] => [pachnu]\n",
            "[im fat] => [jsem tlusty]\n",
            "[im fat] => [jsem tlusta]\n",
            "[im sad] => [jsem smutny]\n",
            "[im sad] => [jsem smutna]\n",
            "[im sad] => [je mi smutno]\n",
            "[im wet] => [jsem mokry]\n",
            "[im wet] => [jsem mokra]\n",
            "[its ok] => [je to ok]\n",
            "[its ok] => [vsechno v poradku]\n",
            "[perfect] => [perfektni]\n",
            "[see you] => [uvidime se]\n",
            "[see you] => [sbohem]\n",
            "[tom ate] => [tom jedl]\n",
            "[wake up] => [vzbudte se]\n",
            "[wake up] => [probud se]\n",
            "[wake up] => [vzbud se]\n",
            "[wake up] => [probudte se]\n",
            "[wake up] => [vzbud se]\n",
            "[wash up] => [umyj se]\n",
            "[we know] => [vime]\n",
            "[we know] => [my vime]\n",
            "[we lost] => [prohrali jsme]\n",
            "[who won] => [kdo zvitezil]\n",
            "[who won] => [kdo vyhral]\n",
            "[am i fat] => [jsem tlusta]\n",
            "[am i fat] => [jsem tlusty]\n",
            "[ask them] => [zeptej se jich]\n",
            "[ask them] => [popros je]\n",
            "[be a man] => [bud chlap]\n",
            "[catch me] => [chyt si me]\n",
            "[get away] => [vypadni]\n",
            "[how cute] => [jak rozkosne]\n",
            "[i can go] => [muzu jit]\n",
            "[i sat up] => [narovnal jsem se]\n",
            "[i sat up] => [posadil jsem se]\n",
            "[i use it] => [pouzivam to]\n",
            "[im rich] => [jsem bohaty]\n",
            "[im rich] => [jsem bohata]\n",
            "[im ugly] => [jsem oskliva]\n",
            "[lets go] => [uz pojdme]\n",
            "[look out] => [opatrne]\n",
            "[she lied] => [lhala]\n",
            "[sit here] => [sednete si tady]\n",
            "[speak up] => [mluv hlasiteji]\n",
            "[tom lies] => [tom lze]\n",
            "[tom lost] => [tom prohral]\n",
            "[try this] => [zkus tohle]\n",
            "[use this] => [pouzij tohle]\n",
            "[we agree] => [souhlasime]\n",
            "[we stood] => [stali jsme]\n",
            "[after you] => [az po tobe]\n",
            "[answer me] => [odpovez mi]\n",
            "[birds fly] => [ptaci letaji]\n",
            "[come back] => [vrat se zpatky]\n",
            "[come back] => [vratte se]\n",
            "[dogs bark] => [psi stekaji]\n",
            "[dont cry] => [nebrec]\n",
            "[dont cry] => [neplacte]\n",
            "[dont lie] => [nelzi]\n",
            "[feel this] => [sahni si na tohle]\n",
            "[forget it] => [na to zapomen]\n",
            "[get ready] => [priprav se]\n",
            "[go inside] => [jdi dovnitr]\n",
            "[go slower] => [bez pomaleji]\n",
            "[go slower] => [jed pomaleji]\n",
            "[grab this] => [uchop toto]\n",
            "[grab this] => [popadni tohle]\n",
            "[he smiled] => [usmal se]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s41D9n-tzxWY",
        "outputId": "b7d8825c-ecd7-416e-8ee9-a524a3fd3cdb"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-Czech.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-Czech-both.pkl')\n",
        "save_clean_data(train, 'english-Czech-train.pkl')\n",
        "save_clean_data(test, 'english-Czech-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-Czech-both.pkl\n",
            "Saved: english-Czech-train.pkl\n",
            "Saved: english-Czech-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tysbn7pj2_IN",
        "outputId": "403e1822-4713-4bcf-cd66-599b8fb40936"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Czech-both.pkl')\n",
        "train = load_clean_sentences('english-Czech-train.pkl')\n",
        "test = load_clean_sentences('english-Czech-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print(' Czech Vocabulary Size: %d' % ger_vocab_size)\n",
        "print(' Czech Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3257\n",
            "English Max Length: 8\n",
            " Czech Vocabulary Size: 6536\n",
            " Czech Max Length: 9\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 9, 256)            1673216   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 8, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 8, 3257)           837049    \n",
            "=================================================================\n",
            "Total params: 3,560,889\n",
            "Trainable params: 3,560,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "141/141 - 39s - loss: 4.2025 - val_loss: 3.3427\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.34266, saving model to model.h5\n",
            "Epoch 2/100\n",
            "141/141 - 32s - loss: 3.2661 - val_loss: 3.1565\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.34266 to 3.15655, saving model to model.h5\n",
            "Epoch 3/100\n",
            "141/141 - 33s - loss: 3.1408 - val_loss: 3.0563\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.15655 to 3.05629, saving model to model.h5\n",
            "Epoch 4/100\n",
            "141/141 - 33s - loss: 3.0436 - val_loss: 2.9881\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.05629 to 2.98813, saving model to model.h5\n",
            "Epoch 5/100\n",
            "141/141 - 33s - loss: 2.9856 - val_loss: 2.9347\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.98813 to 2.93471, saving model to model.h5\n",
            "Epoch 6/100\n",
            "141/141 - 33s - loss: 2.9240 - val_loss: 2.8811\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.93471 to 2.88108, saving model to model.h5\n",
            "Epoch 7/100\n",
            "141/141 - 32s - loss: 2.8521 - val_loss: 2.7993\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.88108 to 2.79929, saving model to model.h5\n",
            "Epoch 8/100\n",
            "141/141 - 32s - loss: 2.7849 - val_loss: 2.7253\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.79929 to 2.72528, saving model to model.h5\n",
            "Epoch 9/100\n",
            "141/141 - 32s - loss: 2.6773 - val_loss: 2.6233\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.72528 to 2.62326, saving model to model.h5\n",
            "Epoch 10/100\n",
            "141/141 - 33s - loss: 2.5754 - val_loss: 2.5248\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.62326 to 2.52481, saving model to model.h5\n",
            "Epoch 11/100\n",
            "141/141 - 33s - loss: 2.4788 - val_loss: 2.4424\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.52481 to 2.44241, saving model to model.h5\n",
            "Epoch 12/100\n",
            "141/141 - 33s - loss: 2.3824 - val_loss: 2.3495\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.44241 to 2.34948, saving model to model.h5\n",
            "Epoch 13/100\n",
            "141/141 - 33s - loss: 2.2926 - val_loss: 2.2668\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.34948 to 2.26679, saving model to model.h5\n",
            "Epoch 14/100\n",
            "141/141 - 33s - loss: 2.1932 - val_loss: 2.1630\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.26679 to 2.16303, saving model to model.h5\n",
            "Epoch 15/100\n",
            "141/141 - 33s - loss: 2.0919 - val_loss: 2.0695\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.16303 to 2.06949, saving model to model.h5\n",
            "Epoch 16/100\n",
            "141/141 - 33s - loss: 1.9990 - val_loss: 1.9890\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.06949 to 1.98895, saving model to model.h5\n",
            "Epoch 17/100\n",
            "141/141 - 33s - loss: 1.9102 - val_loss: 1.9097\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.98895 to 1.90966, saving model to model.h5\n",
            "Epoch 18/100\n",
            "141/141 - 33s - loss: 1.8220 - val_loss: 1.8308\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.90966 to 1.83082, saving model to model.h5\n",
            "Epoch 19/100\n",
            "141/141 - 32s - loss: 1.7357 - val_loss: 1.7511\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.83082 to 1.75107, saving model to model.h5\n",
            "Epoch 20/100\n",
            "141/141 - 33s - loss: 1.6492 - val_loss: 1.6776\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.75107 to 1.67763, saving model to model.h5\n",
            "Epoch 21/100\n",
            "141/141 - 33s - loss: 1.5677 - val_loss: 1.6026\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.67763 to 1.60260, saving model to model.h5\n",
            "Epoch 22/100\n",
            "141/141 - 33s - loss: 1.4868 - val_loss: 1.5274\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.60260 to 1.52738, saving model to model.h5\n",
            "Epoch 23/100\n",
            "141/141 - 33s - loss: 1.4087 - val_loss: 1.4606\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.52738 to 1.46061, saving model to model.h5\n",
            "Epoch 24/100\n",
            "141/141 - 33s - loss: 1.3285 - val_loss: 1.3903\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.46061 to 1.39034, saving model to model.h5\n",
            "Epoch 25/100\n",
            "141/141 - 33s - loss: 1.2580 - val_loss: 1.3439\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.39034 to 1.34393, saving model to model.h5\n",
            "Epoch 26/100\n",
            "141/141 - 33s - loss: 1.1886 - val_loss: 1.2692\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.34393 to 1.26917, saving model to model.h5\n",
            "Epoch 27/100\n",
            "141/141 - 33s - loss: 1.1173 - val_loss: 1.2093\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.26917 to 1.20934, saving model to model.h5\n",
            "Epoch 28/100\n",
            "141/141 - 33s - loss: 1.0517 - val_loss: 1.1521\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.20934 to 1.15214, saving model to model.h5\n",
            "Epoch 29/100\n",
            "141/141 - 32s - loss: 0.9870 - val_loss: 1.0975\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.15214 to 1.09750, saving model to model.h5\n",
            "Epoch 30/100\n",
            "141/141 - 32s - loss: 0.9237 - val_loss: 1.0488\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.09750 to 1.04882, saving model to model.h5\n",
            "Epoch 31/100\n",
            "141/141 - 32s - loss: 0.8644 - val_loss: 0.9990\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.04882 to 0.99896, saving model to model.h5\n",
            "Epoch 32/100\n",
            "141/141 - 33s - loss: 0.8110 - val_loss: 0.9536\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.99896 to 0.95361, saving model to model.h5\n",
            "Epoch 33/100\n",
            "141/141 - 33s - loss: 0.7574 - val_loss: 0.9111\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.95361 to 0.91108, saving model to model.h5\n",
            "Epoch 34/100\n",
            "141/141 - 33s - loss: 0.7092 - val_loss: 0.8743\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.91108 to 0.87426, saving model to model.h5\n",
            "Epoch 35/100\n",
            "141/141 - 33s - loss: 0.6593 - val_loss: 0.8298\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.87426 to 0.82980, saving model to model.h5\n",
            "Epoch 36/100\n",
            "141/141 - 32s - loss: 0.6135 - val_loss: 0.7942\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.82980 to 0.79423, saving model to model.h5\n",
            "Epoch 37/100\n",
            "141/141 - 32s - loss: 0.5725 - val_loss: 0.7696\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.79423 to 0.76964, saving model to model.h5\n",
            "Epoch 38/100\n",
            "141/141 - 33s - loss: 0.5344 - val_loss: 0.7408\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.76964 to 0.74078, saving model to model.h5\n",
            "Epoch 39/100\n",
            "141/141 - 32s - loss: 0.4968 - val_loss: 0.7049\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.74078 to 0.70485, saving model to model.h5\n",
            "Epoch 40/100\n",
            "141/141 - 33s - loss: 0.4585 - val_loss: 0.6765\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.70485 to 0.67649, saving model to model.h5\n",
            "Epoch 41/100\n",
            "141/141 - 33s - loss: 0.4236 - val_loss: 0.6473\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.67649 to 0.64731, saving model to model.h5\n",
            "Epoch 42/100\n",
            "141/141 - 33s - loss: 0.3935 - val_loss: 0.6236\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.64731 to 0.62357, saving model to model.h5\n",
            "Epoch 43/100\n",
            "141/141 - 33s - loss: 0.3658 - val_loss: 0.6023\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.62357 to 0.60230, saving model to model.h5\n",
            "Epoch 44/100\n",
            "141/141 - 33s - loss: 0.3421 - val_loss: 0.5851\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.60230 to 0.58506, saving model to model.h5\n",
            "Epoch 45/100\n",
            "141/141 - 32s - loss: 0.3142 - val_loss: 0.5620\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.58506 to 0.56196, saving model to model.h5\n",
            "Epoch 46/100\n",
            "141/141 - 33s - loss: 0.2919 - val_loss: 0.5488\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.56196 to 0.54878, saving model to model.h5\n",
            "Epoch 47/100\n",
            "141/141 - 32s - loss: 0.2685 - val_loss: 0.5352\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.54878 to 0.53520, saving model to model.h5\n",
            "Epoch 48/100\n",
            "141/141 - 33s - loss: 0.2496 - val_loss: 0.5225\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.53520 to 0.52249, saving model to model.h5\n",
            "Epoch 49/100\n",
            "141/141 - 32s - loss: 0.2328 - val_loss: 0.5081\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.52249 to 0.50814, saving model to model.h5\n",
            "Epoch 50/100\n",
            "141/141 - 33s - loss: 0.2153 - val_loss: 0.4967\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.50814 to 0.49673, saving model to model.h5\n",
            "Epoch 51/100\n",
            "141/141 - 33s - loss: 0.1997 - val_loss: 0.4808\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.49673 to 0.48079, saving model to model.h5\n",
            "Epoch 52/100\n",
            "141/141 - 33s - loss: 0.1814 - val_loss: 0.4704\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.48079 to 0.47035, saving model to model.h5\n",
            "Epoch 53/100\n",
            "141/141 - 32s - loss: 0.1664 - val_loss: 0.4640\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.47035 to 0.46398, saving model to model.h5\n",
            "Epoch 54/100\n",
            "141/141 - 33s - loss: 0.1536 - val_loss: 0.4530\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.46398 to 0.45298, saving model to model.h5\n",
            "Epoch 55/100\n",
            "141/141 - 33s - loss: 0.1446 - val_loss: 0.4458\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.45298 to 0.44585, saving model to model.h5\n",
            "Epoch 56/100\n",
            "141/141 - 32s - loss: 0.1329 - val_loss: 0.4389\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.44585 to 0.43894, saving model to model.h5\n",
            "Epoch 57/100\n",
            "141/141 - 32s - loss: 0.1208 - val_loss: 0.4316\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.43894 to 0.43157, saving model to model.h5\n",
            "Epoch 58/100\n",
            "141/141 - 32s - loss: 0.1124 - val_loss: 0.4237\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.43157 to 0.42367, saving model to model.h5\n",
            "Epoch 59/100\n",
            "141/141 - 33s - loss: 0.1031 - val_loss: 0.4208\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.42367 to 0.42083, saving model to model.h5\n",
            "Epoch 60/100\n",
            "141/141 - 33s - loss: 0.0944 - val_loss: 0.4129\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.42083 to 0.41285, saving model to model.h5\n",
            "Epoch 61/100\n",
            "141/141 - 33s - loss: 0.0872 - val_loss: 0.4095\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.41285 to 0.40945, saving model to model.h5\n",
            "Epoch 62/100\n",
            "141/141 - 33s - loss: 0.0823 - val_loss: 0.4087\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.40945 to 0.40870, saving model to model.h5\n",
            "Epoch 63/100\n",
            "141/141 - 33s - loss: 0.0786 - val_loss: 0.4129\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.40870\n",
            "Epoch 64/100\n",
            "141/141 - 33s - loss: 0.0771 - val_loss: 0.4069\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.40870 to 0.40689, saving model to model.h5\n",
            "Epoch 65/100\n",
            "141/141 - 32s - loss: 0.0741 - val_loss: 0.4063\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.40689 to 0.40628, saving model to model.h5\n",
            "Epoch 66/100\n",
            "141/141 - 33s - loss: 0.0698 - val_loss: 0.4027\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.40628 to 0.40273, saving model to model.h5\n",
            "Epoch 67/100\n",
            "141/141 - 33s - loss: 0.0669 - val_loss: 0.4024\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.40273 to 0.40242, saving model to model.h5\n",
            "Epoch 68/100\n",
            "141/141 - 33s - loss: 0.0621 - val_loss: 0.3982\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.40242 to 0.39820, saving model to model.h5\n",
            "Epoch 69/100\n",
            "141/141 - 33s - loss: 0.0566 - val_loss: 0.3936\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.39820 to 0.39361, saving model to model.h5\n",
            "Epoch 70/100\n",
            "141/141 - 33s - loss: 0.0505 - val_loss: 0.3921\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.39361 to 0.39207, saving model to model.h5\n",
            "Epoch 71/100\n",
            "141/141 - 32s - loss: 0.0473 - val_loss: 0.3875\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.39207 to 0.38751, saving model to model.h5\n",
            "Epoch 72/100\n",
            "141/141 - 32s - loss: 0.0426 - val_loss: 0.3873\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.38751 to 0.38731, saving model to model.h5\n",
            "Epoch 73/100\n",
            "141/141 - 32s - loss: 0.0392 - val_loss: 0.3855\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.38731 to 0.38548, saving model to model.h5\n",
            "Epoch 74/100\n",
            "141/141 - 32s - loss: 0.0358 - val_loss: 0.3843\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.38548 to 0.38429, saving model to model.h5\n",
            "Epoch 75/100\n",
            "141/141 - 32s - loss: 0.0352 - val_loss: 0.3842\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.38429 to 0.38415, saving model to model.h5\n",
            "Epoch 76/100\n",
            "141/141 - 32s - loss: 0.0327 - val_loss: 0.3848\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.38415\n",
            "Epoch 77/100\n",
            "141/141 - 33s - loss: 0.0308 - val_loss: 0.3860\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.38415\n",
            "Epoch 78/100\n",
            "141/141 - 33s - loss: 0.0298 - val_loss: 0.3864\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.38415\n",
            "Epoch 79/100\n",
            "141/141 - 33s - loss: 0.0322 - val_loss: 0.3905\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.38415\n",
            "Epoch 80/100\n",
            "141/141 - 33s - loss: 0.0439 - val_loss: 0.4213\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.38415\n",
            "Epoch 81/100\n",
            "141/141 - 32s - loss: 0.1028 - val_loss: 0.4762\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.38415\n",
            "Epoch 82/100\n",
            "141/141 - 32s - loss: 0.1361 - val_loss: 0.4472\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.38415\n",
            "Epoch 83/100\n",
            "141/141 - 33s - loss: 0.0832 - val_loss: 0.4045\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.38415\n",
            "Epoch 84/100\n",
            "141/141 - 33s - loss: 0.0428 - val_loss: 0.3878\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.38415\n",
            "Epoch 85/100\n",
            "141/141 - 32s - loss: 0.0291 - val_loss: 0.3815\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.38415 to 0.38154, saving model to model.h5\n",
            "Epoch 86/100\n",
            "141/141 - 33s - loss: 0.0234 - val_loss: 0.3799\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.38154 to 0.37985, saving model to model.h5\n",
            "Epoch 87/100\n",
            "141/141 - 33s - loss: 0.0207 - val_loss: 0.3804\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.37985\n",
            "Epoch 88/100\n",
            "141/141 - 33s - loss: 0.0200 - val_loss: 0.3801\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.37985\n",
            "Epoch 89/100\n",
            "141/141 - 33s - loss: 0.0193 - val_loss: 0.3806\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.37985\n",
            "Epoch 90/100\n",
            "141/141 - 32s - loss: 0.0191 - val_loss: 0.3801\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.37985\n",
            "Epoch 91/100\n",
            "141/141 - 33s - loss: 0.0183 - val_loss: 0.3821\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.37985\n",
            "Epoch 92/100\n",
            "141/141 - 33s - loss: 0.0182 - val_loss: 0.3813\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.37985\n",
            "Epoch 93/100\n",
            "141/141 - 33s - loss: 0.0180 - val_loss: 0.3829\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.37985\n",
            "Epoch 94/100\n",
            "141/141 - 33s - loss: 0.0177 - val_loss: 0.3839\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.37985\n",
            "Epoch 95/100\n",
            "141/141 - 33s - loss: 0.0177 - val_loss: 0.3841\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.37985\n",
            "Epoch 96/100\n",
            "141/141 - 33s - loss: 0.0175 - val_loss: 0.3849\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.37985\n",
            "Epoch 97/100\n",
            "141/141 - 33s - loss: 0.0173 - val_loss: 0.3845\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.37985\n",
            "Epoch 98/100\n",
            "141/141 - 34s - loss: 0.0174 - val_loss: 0.3859\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.37985\n",
            "Epoch 99/100\n",
            "141/141 - 33s - loss: 0.0175 - val_loss: 0.3857\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.37985\n",
            "Epoch 100/100\n",
            "141/141 - 33s - loss: 0.0175 - val_loss: 0.3878\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.37985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d0a7c5e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3XRXSM3Axzh",
        "outputId": "264b91df-3488-41a6-e1ad-67b649ea61f6"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Czech-both.pkl')\n",
        "train = load_clean_sentences('english-Czech-train.pkl')\n",
        "test = load_clean_sentences('english-Czech-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print(' Czech Vocabulary Size: %d' % ger_vocab_size)\n",
        "print(' Czech Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=200, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 3257\n",
            "English Max Length: 8\n",
            " Czech Vocabulary Size: 6536\n",
            " Czech Max Length: 9\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 9, 256)            1673216   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 8, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 8, 3257)           837049    \n",
            "=================================================================\n",
            "Total params: 3,560,889\n",
            "Trainable params: 3,560,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "141/141 - 50s - loss: 4.1771 - val_loss: 3.3345\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.33449, saving model to model.h5\n",
            "Epoch 2/200\n",
            "141/141 - 43s - loss: 3.2373 - val_loss: 3.1382\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.33449 to 3.13816, saving model to model.h5\n",
            "Epoch 3/200\n",
            "141/141 - 43s - loss: 3.1195 - val_loss: 3.0515\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.13816 to 3.05151, saving model to model.h5\n",
            "Epoch 4/200\n",
            "141/141 - 45s - loss: 3.0299 - val_loss: 2.9730\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.05151 to 2.97296, saving model to model.h5\n",
            "Epoch 5/200\n",
            "141/141 - 50s - loss: 2.9624 - val_loss: 2.9195\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.97296 to 2.91950, saving model to model.h5\n",
            "Epoch 6/200\n",
            "141/141 - 43s - loss: 2.9111 - val_loss: 2.8601\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.91950 to 2.86007, saving model to model.h5\n",
            "Epoch 7/200\n",
            "141/141 - 43s - loss: 2.8305 - val_loss: 2.7686\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.86007 to 2.76863, saving model to model.h5\n",
            "Epoch 8/200\n",
            "141/141 - 43s - loss: 2.7305 - val_loss: 2.6619\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.76863 to 2.66193, saving model to model.h5\n",
            "Epoch 9/200\n",
            "141/141 - 43s - loss: 2.6215 - val_loss: 2.5694\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.66193 to 2.56937, saving model to model.h5\n",
            "Epoch 10/200\n",
            "141/141 - 43s - loss: 2.5256 - val_loss: 2.4732\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.56937 to 2.47316, saving model to model.h5\n",
            "Epoch 11/200\n",
            "141/141 - 43s - loss: 2.4301 - val_loss: 2.3811\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.47316 to 2.38113, saving model to model.h5\n",
            "Epoch 12/200\n",
            "141/141 - 43s - loss: 2.3292 - val_loss: 2.2870\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.38113 to 2.28701, saving model to model.h5\n",
            "Epoch 13/200\n",
            "141/141 - 43s - loss: 2.2256 - val_loss: 2.1930\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.28701 to 2.19301, saving model to model.h5\n",
            "Epoch 14/200\n",
            "141/141 - 43s - loss: 2.1227 - val_loss: 2.0912\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.19301 to 2.09123, saving model to model.h5\n",
            "Epoch 15/200\n",
            "141/141 - 44s - loss: 2.0201 - val_loss: 1.9927\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.09123 to 1.99273, saving model to model.h5\n",
            "Epoch 16/200\n",
            "141/141 - 44s - loss: 1.9196 - val_loss: 1.9057\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.99273 to 1.90571, saving model to model.h5\n",
            "Epoch 17/200\n",
            "141/141 - 44s - loss: 1.8206 - val_loss: 1.8412\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.90571 to 1.84118, saving model to model.h5\n",
            "Epoch 18/200\n",
            "141/141 - 44s - loss: 1.7285 - val_loss: 1.7286\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.84118 to 1.72860, saving model to model.h5\n",
            "Epoch 19/200\n",
            "141/141 - 43s - loss: 1.6364 - val_loss: 1.6483\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.72860 to 1.64825, saving model to model.h5\n",
            "Epoch 20/200\n",
            "141/141 - 43s - loss: 1.5493 - val_loss: 1.5859\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.64825 to 1.58595, saving model to model.h5\n",
            "Epoch 21/200\n",
            "141/141 - 43s - loss: 1.4675 - val_loss: 1.5074\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.58595 to 1.50738, saving model to model.h5\n",
            "Epoch 22/200\n",
            "141/141 - 43s - loss: 1.3855 - val_loss: 1.4246\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.50738 to 1.42462, saving model to model.h5\n",
            "Epoch 23/200\n",
            "141/141 - 43s - loss: 1.3033 - val_loss: 1.3678\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.42462 to 1.36781, saving model to model.h5\n",
            "Epoch 24/200\n",
            "141/141 - 43s - loss: 1.2290 - val_loss: 1.3121\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.36781 to 1.31212, saving model to model.h5\n",
            "Epoch 25/200\n",
            "141/141 - 43s - loss: 1.1581 - val_loss: 1.2287\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.31212 to 1.22869, saving model to model.h5\n",
            "Epoch 26/200\n",
            "141/141 - 43s - loss: 1.0845 - val_loss: 1.1727\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.22869 to 1.17270, saving model to model.h5\n",
            "Epoch 27/200\n",
            "141/141 - 43s - loss: 1.0143 - val_loss: 1.1151\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.17270 to 1.11508, saving model to model.h5\n",
            "Epoch 28/200\n",
            "141/141 - 43s - loss: 0.9508 - val_loss: 1.0553\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.11508 to 1.05534, saving model to model.h5\n",
            "Epoch 29/200\n",
            "141/141 - 43s - loss: 0.8840 - val_loss: 1.0020\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.05534 to 1.00202, saving model to model.h5\n",
            "Epoch 30/200\n",
            "141/141 - 43s - loss: 0.8255 - val_loss: 0.9594\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.00202 to 0.95938, saving model to model.h5\n",
            "Epoch 31/200\n",
            "141/141 - 43s - loss: 0.7652 - val_loss: 0.9094\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.95938 to 0.90944, saving model to model.h5\n",
            "Epoch 32/200\n",
            "141/141 - 43s - loss: 0.7107 - val_loss: 0.8649\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.90944 to 0.86495, saving model to model.h5\n",
            "Epoch 33/200\n",
            "141/141 - 43s - loss: 0.6597 - val_loss: 0.8227\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.86495 to 0.82270, saving model to model.h5\n",
            "Epoch 34/200\n",
            "141/141 - 43s - loss: 0.6145 - val_loss: 0.7866\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.82270 to 0.78663, saving model to model.h5\n",
            "Epoch 35/200\n",
            "141/141 - 43s - loss: 0.5684 - val_loss: 0.7476\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.78663 to 0.74758, saving model to model.h5\n",
            "Epoch 36/200\n",
            "141/141 - 43s - loss: 0.5233 - val_loss: 0.7175\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.74758 to 0.71747, saving model to model.h5\n",
            "Epoch 37/200\n",
            "141/141 - 43s - loss: 0.4848 - val_loss: 0.6881\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.71747 to 0.68815, saving model to model.h5\n",
            "Epoch 38/200\n",
            "141/141 - 43s - loss: 0.4476 - val_loss: 0.6553\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.68815 to 0.65527, saving model to model.h5\n",
            "Epoch 39/200\n",
            "141/141 - 43s - loss: 0.4105 - val_loss: 0.6292\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.65527 to 0.62923, saving model to model.h5\n",
            "Epoch 40/200\n",
            "141/141 - 43s - loss: 0.3781 - val_loss: 0.6083\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.62923 to 0.60827, saving model to model.h5\n",
            "Epoch 41/200\n",
            "141/141 - 43s - loss: 0.3498 - val_loss: 0.5909\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.60827 to 0.59094, saving model to model.h5\n",
            "Epoch 42/200\n",
            "141/141 - 43s - loss: 0.3237 - val_loss: 0.5664\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.59094 to 0.56638, saving model to model.h5\n",
            "Epoch 43/200\n",
            "141/141 - 44s - loss: 0.2979 - val_loss: 0.5401\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.56638 to 0.54012, saving model to model.h5\n",
            "Epoch 44/200\n",
            "141/141 - 43s - loss: 0.2704 - val_loss: 0.5246\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.54012 to 0.52461, saving model to model.h5\n",
            "Epoch 45/200\n",
            "141/141 - 43s - loss: 0.2477 - val_loss: 0.5123\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.52461 to 0.51228, saving model to model.h5\n",
            "Epoch 46/200\n",
            "141/141 - 43s - loss: 0.2267 - val_loss: 0.4934\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.51228 to 0.49339, saving model to model.h5\n",
            "Epoch 47/200\n",
            "141/141 - 43s - loss: 0.2067 - val_loss: 0.4806\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.49339 to 0.48061, saving model to model.h5\n",
            "Epoch 48/200\n",
            "141/141 - 43s - loss: 0.1897 - val_loss: 0.4669\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.48061 to 0.46694, saving model to model.h5\n",
            "Epoch 49/200\n",
            "141/141 - 43s - loss: 0.1736 - val_loss: 0.4604\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.46694 to 0.46041, saving model to model.h5\n",
            "Epoch 50/200\n",
            "141/141 - 43s - loss: 0.1589 - val_loss: 0.4477\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.46041 to 0.44769, saving model to model.h5\n",
            "Epoch 51/200\n",
            "141/141 - 43s - loss: 0.1452 - val_loss: 0.4447\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.44769 to 0.44474, saving model to model.h5\n",
            "Epoch 52/200\n",
            "141/141 - 43s - loss: 0.1375 - val_loss: 0.4391\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.44474 to 0.43914, saving model to model.h5\n",
            "Epoch 53/200\n",
            "141/141 - 43s - loss: 0.1294 - val_loss: 0.4256\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.43914 to 0.42557, saving model to model.h5\n",
            "Epoch 54/200\n",
            "141/141 - 43s - loss: 0.1186 - val_loss: 0.4235\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.42557 to 0.42354, saving model to model.h5\n",
            "Epoch 55/200\n",
            "141/141 - 43s - loss: 0.1081 - val_loss: 0.4167\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.42354 to 0.41666, saving model to model.h5\n",
            "Epoch 56/200\n",
            "141/141 - 43s - loss: 0.0996 - val_loss: 0.4092\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.41666 to 0.40923, saving model to model.h5\n",
            "Epoch 57/200\n",
            "141/141 - 42s - loss: 0.0880 - val_loss: 0.4041\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.40923 to 0.40414, saving model to model.h5\n",
            "Epoch 58/200\n",
            "141/141 - 41s - loss: 0.0790 - val_loss: 0.3987\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.40414 to 0.39872, saving model to model.h5\n",
            "Epoch 59/200\n",
            "141/141 - 42s - loss: 0.0718 - val_loss: 0.3929\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.39872 to 0.39289, saving model to model.h5\n",
            "Epoch 60/200\n",
            "141/141 - 43s - loss: 0.0663 - val_loss: 0.3935\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.39289\n",
            "Epoch 61/200\n",
            "141/141 - 43s - loss: 0.0654 - val_loss: 0.3952\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.39289\n",
            "Epoch 62/200\n",
            "141/141 - 43s - loss: 0.0667 - val_loss: 0.4009\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.39289\n",
            "Epoch 63/200\n",
            "141/141 - 42s - loss: 0.0726 - val_loss: 0.4148\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.39289\n",
            "Epoch 64/200\n",
            "141/141 - 41s - loss: 0.0807 - val_loss: 0.4078\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.39289\n",
            "Epoch 65/200\n",
            "141/141 - 41s - loss: 0.0766 - val_loss: 0.3996\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.39289\n",
            "Epoch 66/200\n",
            "141/141 - 41s - loss: 0.0621 - val_loss: 0.3873\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.39289 to 0.38726, saving model to model.h5\n",
            "Epoch 67/200\n",
            "141/141 - 41s - loss: 0.0506 - val_loss: 0.3809\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.38726 to 0.38090, saving model to model.h5\n",
            "Epoch 68/200\n",
            "141/141 - 40s - loss: 0.0406 - val_loss: 0.3765\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.38090 to 0.37648, saving model to model.h5\n",
            "Epoch 69/200\n",
            "141/141 - 40s - loss: 0.0351 - val_loss: 0.3748\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.37648 to 0.37480, saving model to model.h5\n",
            "Epoch 70/200\n",
            "141/141 - 39s - loss: 0.0327 - val_loss: 0.3748\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.37480 to 0.37477, saving model to model.h5\n",
            "Epoch 71/200\n",
            "141/141 - 39s - loss: 0.0299 - val_loss: 0.3736\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.37477 to 0.37360, saving model to model.h5\n",
            "Epoch 72/200\n",
            "141/141 - 40s - loss: 0.0292 - val_loss: 0.3752\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.37360\n",
            "Epoch 73/200\n",
            "141/141 - 40s - loss: 0.0286 - val_loss: 0.3761\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.37360\n",
            "Epoch 74/200\n",
            "141/141 - 41s - loss: 0.0283 - val_loss: 0.3744\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.37360\n",
            "Epoch 75/200\n",
            "141/141 - 41s - loss: 0.0262 - val_loss: 0.3774\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.37360\n",
            "Epoch 76/200\n",
            "141/141 - 41s - loss: 0.0265 - val_loss: 0.3789\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.37360\n",
            "Epoch 77/200\n",
            "141/141 - 40s - loss: 0.0279 - val_loss: 0.3804\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.37360\n",
            "Epoch 78/200\n",
            "141/141 - 40s - loss: 0.0315 - val_loss: 0.3920\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.37360\n",
            "Epoch 79/200\n",
            "141/141 - 40s - loss: 0.0680 - val_loss: 0.4553\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.37360\n",
            "Epoch 80/200\n",
            "141/141 - 40s - loss: 0.1406 - val_loss: 0.4537\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.37360\n",
            "Epoch 81/200\n",
            "141/141 - 39s - loss: 0.1103 - val_loss: 0.4118\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.37360\n",
            "Epoch 82/200\n",
            "141/141 - 40s - loss: 0.0580 - val_loss: 0.3888\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.37360\n",
            "Epoch 83/200\n",
            "141/141 - 40s - loss: 0.0343 - val_loss: 0.3783\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.37360\n",
            "Epoch 84/200\n",
            "141/141 - 39s - loss: 0.0252 - val_loss: 0.3764\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.37360\n",
            "Epoch 85/200\n",
            "141/141 - 40s - loss: 0.0209 - val_loss: 0.3741\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.37360\n",
            "Epoch 86/200\n",
            "141/141 - 41s - loss: 0.0195 - val_loss: 0.3748\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.37360\n",
            "Epoch 87/200\n",
            "141/141 - 41s - loss: 0.0188 - val_loss: 0.3751\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.37360\n",
            "Epoch 88/200\n",
            "141/141 - 41s - loss: 0.0182 - val_loss: 0.3754\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.37360\n",
            "Epoch 89/200\n",
            "141/141 - 41s - loss: 0.0177 - val_loss: 0.3756\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.37360\n",
            "Epoch 90/200\n",
            "141/141 - 41s - loss: 0.0176 - val_loss: 0.3763\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.37360\n",
            "Epoch 91/200\n",
            "141/141 - 41s - loss: 0.0175 - val_loss: 0.3772\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.37360\n",
            "Epoch 92/200\n",
            "141/141 - 40s - loss: 0.0173 - val_loss: 0.3771\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.37360\n",
            "Epoch 93/200\n",
            "141/141 - 40s - loss: 0.0172 - val_loss: 0.3777\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.37360\n",
            "Epoch 94/200\n",
            "141/141 - 40s - loss: 0.0166 - val_loss: 0.3776\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.37360\n",
            "Epoch 95/200\n",
            "141/141 - 41s - loss: 0.0167 - val_loss: 0.3792\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.37360\n",
            "Epoch 96/200\n",
            "141/141 - 41s - loss: 0.0167 - val_loss: 0.3797\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.37360\n",
            "Epoch 97/200\n",
            "141/141 - 41s - loss: 0.0172 - val_loss: 0.3809\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.37360\n",
            "Epoch 98/200\n",
            "141/141 - 41s - loss: 0.0172 - val_loss: 0.3819\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.37360\n",
            "Epoch 99/200\n",
            "141/141 - 40s - loss: 0.0175 - val_loss: 0.3818\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.37360\n",
            "Epoch 100/200\n",
            "141/141 - 40s - loss: 0.0178 - val_loss: 0.3821\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.37360\n",
            "Epoch 101/200\n",
            "141/141 - 40s - loss: 0.0192 - val_loss: 0.3841\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.37360\n",
            "Epoch 102/200\n",
            "141/141 - 40s - loss: 0.0521 - val_loss: 0.4768\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.37360\n",
            "Epoch 103/200\n",
            "141/141 - 40s - loss: 0.1297 - val_loss: 0.4438\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.37360\n",
            "Epoch 104/200\n",
            "141/141 - 40s - loss: 0.0803 - val_loss: 0.4095\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.37360\n",
            "Epoch 105/200\n",
            "141/141 - 40s - loss: 0.0402 - val_loss: 0.3903\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.37360\n",
            "Epoch 106/200\n",
            "141/141 - 40s - loss: 0.0233 - val_loss: 0.3843\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.37360\n",
            "Epoch 107/200\n",
            "141/141 - 39s - loss: 0.0173 - val_loss: 0.3818\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.37360\n",
            "Epoch 108/200\n",
            "141/141 - 39s - loss: 0.0158 - val_loss: 0.3826\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.37360\n",
            "Epoch 109/200\n",
            "141/141 - 39s - loss: 0.0149 - val_loss: 0.3825\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.37360\n",
            "Epoch 110/200\n",
            "141/141 - 40s - loss: 0.0147 - val_loss: 0.3829\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.37360\n",
            "Epoch 111/200\n",
            "141/141 - 39s - loss: 0.0143 - val_loss: 0.3833\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.37360\n",
            "Epoch 112/200\n",
            "141/141 - 39s - loss: 0.0139 - val_loss: 0.3828\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.37360\n",
            "Epoch 113/200\n",
            "141/141 - 39s - loss: 0.0141 - val_loss: 0.3837\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.37360\n",
            "Epoch 114/200\n",
            "141/141 - 39s - loss: 0.0140 - val_loss: 0.3846\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.37360\n",
            "Epoch 115/200\n",
            "141/141 - 40s - loss: 0.0140 - val_loss: 0.3845\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.37360\n",
            "Epoch 116/200\n",
            "141/141 - 39s - loss: 0.0140 - val_loss: 0.3852\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.37360\n",
            "Epoch 117/200\n",
            "141/141 - 40s - loss: 0.0140 - val_loss: 0.3850\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.37360\n",
            "Epoch 118/200\n",
            "141/141 - 40s - loss: 0.0142 - val_loss: 0.3856\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.37360\n",
            "Epoch 119/200\n",
            "141/141 - 40s - loss: 0.0139 - val_loss: 0.3868\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.37360\n",
            "Epoch 120/200\n",
            "141/141 - 40s - loss: 0.0138 - val_loss: 0.3864\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.37360\n",
            "Epoch 121/200\n",
            "141/141 - 40s - loss: 0.0142 - val_loss: 0.3873\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.37360\n",
            "Epoch 122/200\n",
            "141/141 - 40s - loss: 0.0142 - val_loss: 0.3875\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.37360\n",
            "Epoch 123/200\n",
            "141/141 - 41s - loss: 0.0141 - val_loss: 0.3875\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.37360\n",
            "Epoch 124/200\n",
            "141/141 - 41s - loss: 0.0148 - val_loss: 0.3886\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.37360\n",
            "Epoch 125/200\n",
            "141/141 - 40s - loss: 0.0183 - val_loss: 0.4009\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.37360\n",
            "Epoch 126/200\n",
            "141/141 - 41s - loss: 0.0639 - val_loss: 0.4879\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.37360\n",
            "Epoch 127/200\n",
            "141/141 - 41s - loss: 0.1106 - val_loss: 0.4361\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.37360\n",
            "Epoch 128/200\n",
            "141/141 - 41s - loss: 0.0556 - val_loss: 0.4049\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.37360\n",
            "Epoch 129/200\n",
            "141/141 - 41s - loss: 0.0281 - val_loss: 0.3911\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.37360\n",
            "Epoch 130/200\n",
            "141/141 - 41s - loss: 0.0181 - val_loss: 0.3878\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.37360\n",
            "Epoch 131/200\n",
            "141/141 - 41s - loss: 0.0147 - val_loss: 0.3871\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.37360\n",
            "Epoch 132/200\n",
            "141/141 - 41s - loss: 0.0135 - val_loss: 0.3870\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.37360\n",
            "Epoch 133/200\n",
            "141/141 - 41s - loss: 0.0132 - val_loss: 0.3871\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.37360\n",
            "Epoch 134/200\n",
            "141/141 - 41s - loss: 0.0130 - val_loss: 0.3868\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.37360\n",
            "Epoch 135/200\n",
            "141/141 - 41s - loss: 0.0130 - val_loss: 0.3872\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.37360\n",
            "Epoch 136/200\n",
            "141/141 - 41s - loss: 0.0129 - val_loss: 0.3870\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.37360\n",
            "Epoch 137/200\n",
            "141/141 - 41s - loss: 0.0125 - val_loss: 0.3881\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.37360\n",
            "Epoch 138/200\n",
            "141/141 - 41s - loss: 0.0127 - val_loss: 0.3878\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.37360\n",
            "Epoch 139/200\n",
            "141/141 - 41s - loss: 0.0127 - val_loss: 0.3888\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.37360\n",
            "Epoch 140/200\n",
            "141/141 - 41s - loss: 0.0126 - val_loss: 0.3889\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.37360\n",
            "Epoch 141/200\n",
            "141/141 - 41s - loss: 0.0124 - val_loss: 0.3891\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.37360\n",
            "Epoch 142/200\n",
            "141/141 - 41s - loss: 0.0127 - val_loss: 0.3893\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.37360\n",
            "Epoch 143/200\n",
            "141/141 - 41s - loss: 0.0127 - val_loss: 0.3907\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.37360\n",
            "Epoch 144/200\n",
            "141/141 - 40s - loss: 0.0130 - val_loss: 0.3907\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.37360\n",
            "Epoch 145/200\n",
            "141/141 - 40s - loss: 0.0128 - val_loss: 0.3909\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.37360\n",
            "Epoch 146/200\n",
            "141/141 - 40s - loss: 0.0131 - val_loss: 0.3908\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.37360\n",
            "Epoch 147/200\n",
            "141/141 - 40s - loss: 0.0131 - val_loss: 0.3909\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.37360\n",
            "Epoch 148/200\n",
            "141/141 - 41s - loss: 0.0132 - val_loss: 0.3912\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.37360\n",
            "Epoch 149/200\n",
            "141/141 - 41s - loss: 0.0131 - val_loss: 0.3927\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.37360\n",
            "Epoch 150/200\n",
            "141/141 - 40s - loss: 0.0136 - val_loss: 0.3932\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.37360\n",
            "Epoch 151/200\n",
            "141/141 - 40s - loss: 0.0144 - val_loss: 0.3950\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.37360\n",
            "Epoch 152/200\n",
            "141/141 - 40s - loss: 0.0377 - val_loss: 0.4816\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.37360\n",
            "Epoch 153/200\n",
            "141/141 - 40s - loss: 0.1157 - val_loss: 0.4427\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.37360\n",
            "Epoch 154/200\n",
            "141/141 - 41s - loss: 0.0599 - val_loss: 0.4097\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.37360\n",
            "Epoch 155/200\n",
            "141/141 - 41s - loss: 0.0255 - val_loss: 0.3957\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.37360\n",
            "Epoch 156/200\n",
            "141/141 - 41s - loss: 0.0158 - val_loss: 0.3933\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.37360\n",
            "Epoch 157/200\n",
            "141/141 - 40s - loss: 0.0132 - val_loss: 0.3933\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.37360\n",
            "Epoch 158/200\n",
            "141/141 - 40s - loss: 0.0124 - val_loss: 0.3926\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.37360\n",
            "Epoch 159/200\n",
            "141/141 - 40s - loss: 0.0122 - val_loss: 0.3930\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.37360\n",
            "Epoch 160/200\n",
            "141/141 - 40s - loss: 0.0120 - val_loss: 0.3926\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.37360\n",
            "Epoch 161/200\n",
            "141/141 - 40s - loss: 0.0122 - val_loss: 0.3933\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.37360\n",
            "Epoch 162/200\n",
            "141/141 - 40s - loss: 0.0119 - val_loss: 0.3933\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.37360\n",
            "Epoch 163/200\n",
            "141/141 - 40s - loss: 0.0119 - val_loss: 0.3931\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.37360\n",
            "Epoch 164/200\n",
            "141/141 - 40s - loss: 0.0119 - val_loss: 0.3935\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.37360\n",
            "Epoch 165/200\n",
            "141/141 - 41s - loss: 0.0117 - val_loss: 0.3939\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.37360\n",
            "Epoch 166/200\n",
            "141/141 - 40s - loss: 0.0118 - val_loss: 0.3938\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.37360\n",
            "Epoch 167/200\n",
            "141/141 - 40s - loss: 0.0118 - val_loss: 0.3944\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.37360\n",
            "Epoch 168/200\n",
            "141/141 - 40s - loss: 0.0119 - val_loss: 0.3941\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.37360\n",
            "Epoch 169/200\n",
            "141/141 - 41s - loss: 0.0120 - val_loss: 0.3955\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.37360\n",
            "Epoch 170/200\n",
            "141/141 - 41s - loss: 0.0120 - val_loss: 0.3955\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.37360\n",
            "Epoch 171/200\n",
            "141/141 - 41s - loss: 0.0121 - val_loss: 0.3955\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.37360\n",
            "Epoch 172/200\n",
            "141/141 - 41s - loss: 0.0121 - val_loss: 0.3956\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.37360\n",
            "Epoch 173/200\n",
            "141/141 - 41s - loss: 0.0121 - val_loss: 0.3955\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.37360\n",
            "Epoch 174/200\n",
            "141/141 - 41s - loss: 0.0122 - val_loss: 0.3968\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.37360\n",
            "Epoch 175/200\n",
            "141/141 - 41s - loss: 0.0123 - val_loss: 0.3961\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.37360\n",
            "Epoch 176/200\n",
            "141/141 - 41s - loss: 0.0126 - val_loss: 0.3983\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.37360\n",
            "Epoch 177/200\n",
            "141/141 - 41s - loss: 0.0228 - val_loss: 0.4388\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.37360\n",
            "Epoch 178/200\n",
            "141/141 - 41s - loss: 0.0760 - val_loss: 0.4472\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.37360\n",
            "Epoch 179/200\n",
            "141/141 - 41s - loss: 0.0593 - val_loss: 0.4170\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.37360\n",
            "Epoch 180/200\n",
            "141/141 - 41s - loss: 0.0319 - val_loss: 0.3999\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.37360\n",
            "Epoch 181/200\n",
            "141/141 - 41s - loss: 0.0178 - val_loss: 0.3942\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.37360\n",
            "Epoch 182/200\n",
            "141/141 - 41s - loss: 0.0140 - val_loss: 0.3948\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.37360\n",
            "Epoch 183/200\n",
            "141/141 - 41s - loss: 0.0126 - val_loss: 0.3939\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.37360\n",
            "Epoch 184/200\n",
            "141/141 - 41s - loss: 0.0121 - val_loss: 0.3942\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.37360\n",
            "Epoch 185/200\n",
            "141/141 - 41s - loss: 0.0118 - val_loss: 0.3948\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.37360\n",
            "Epoch 186/200\n",
            "141/141 - 41s - loss: 0.0116 - val_loss: 0.3941\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.37360\n",
            "Epoch 187/200\n",
            "141/141 - 41s - loss: 0.0115 - val_loss: 0.3951\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.37360\n",
            "Epoch 188/200\n",
            "141/141 - 41s - loss: 0.0115 - val_loss: 0.3947\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.37360\n",
            "Epoch 189/200\n",
            "141/141 - 41s - loss: 0.0114 - val_loss: 0.3950\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.37360\n",
            "Epoch 190/200\n",
            "141/141 - 41s - loss: 0.0115 - val_loss: 0.3954\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.37360\n",
            "Epoch 191/200\n",
            "141/141 - 42s - loss: 0.0114 - val_loss: 0.3956\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.37360\n",
            "Epoch 192/200\n",
            "141/141 - 41s - loss: 0.0113 - val_loss: 0.3959\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.37360\n",
            "Epoch 193/200\n",
            "141/141 - 41s - loss: 0.0116 - val_loss: 0.3957\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.37360\n",
            "Epoch 194/200\n",
            "141/141 - 41s - loss: 0.0113 - val_loss: 0.3961\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.37360\n",
            "Epoch 195/200\n",
            "141/141 - 41s - loss: 0.0117 - val_loss: 0.3973\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.37360\n",
            "Epoch 196/200\n",
            "141/141 - 41s - loss: 0.0116 - val_loss: 0.3967\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.37360\n",
            "Epoch 197/200\n",
            "141/141 - 41s - loss: 0.0119 - val_loss: 0.3979\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.37360\n",
            "Epoch 198/200\n",
            "141/141 - 41s - loss: 0.0117 - val_loss: 0.3976\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.37360\n",
            "Epoch 199/200\n",
            "141/141 - 41s - loss: 0.0116 - val_loss: 0.3983\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.37360\n",
            "Epoch 200/200\n",
            "141/141 - 41s - loss: 0.0119 - val_loss: 0.3967\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.37360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1bbcd71610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CFGtU_d2dLk",
        "outputId": "bde8dc38-f82f-40ca-e1db-1b95bf89afdd"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Czech-both.pkl')\n",
        "train = load_clean_sentences('english-Czech-train.pkl')\n",
        "test = load_clean_sentences('english-Czech-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[ty knihy zde jsou moje], target=[the books here are mine], predicted=[the books here are mine]\n",
            "src=[kdo te naucil psat], target=[who taught you to write], predicted=[who taught you to write]\n",
            "src=[pustte ty psy], target=[release the dogs], predicted=[release the dogs]\n",
            "src=[nas pes kousne zridka], target=[our dog seldom bites], predicted=[our dog seldom bites]\n",
            "src=[dosly nam moznosti volby], target=[were out of options], predicted=[were out of options]\n",
            "src=[nech si ten papir], target=[keep the paper], predicted=[keep the paper]\n",
            "src=[hrajete squash], target=[do you play squash], predicted=[do you play squash]\n",
            "src=[privedl jsem pritele], target=[i brought a friend], predicted=[i brought a friend]\n",
            "src=[prelozili text], target=[they translated the text], predicted=[they translated the text]\n",
            "src=[proc to tom udelal], target=[what made tom do it], predicted=[what made tom do it]\n",
            "BLEU-1: 0.993435\n",
            "BLEU-2: 0.990843\n",
            "BLEU-3: 0.985334\n",
            "BLEU-4: 0.945630\n",
            "test\n",
            "src=[tom se nevzdal], target=[tom didnt give up], predicted=[tom didnt give up]\n",
            "src=[zdavalo se mi o tobe], target=[i used to dream about you], predicted=[i used to dream about you]\n",
            "src=[nespi], target=[he doesnt sleep], predicted=[he doesnt sleep]\n",
            "src=[tom se rychle osprchoval], target=[tom took a quick shower], predicted=[tom took a quick shower]\n",
            "src=[tom mi opravil esej], target=[tom corrected my essay], predicted=[tom corrected my essay]\n",
            "src=[bylo to velmi dobre], target=[that was very good], predicted=[that was very good]\n",
            "src=[kde je ta brana], target=[where is the gate], predicted=[where is the gate]\n",
            "src=[hodne te obdivuji], target=[i admire you a lot], predicted=[i admire you a lot]\n",
            "src=[potrebuju abys poslouchal], target=[i need you to listen], predicted=[i need you to listen]\n",
            "src=[tom mi jeden poslal], target=[tom sent one to me], predicted=[tom sent one to me]\n",
            "BLEU-1: 0.932431\n",
            "BLEU-2: 0.918535\n",
            "BLEU-3: 0.914338\n",
            "BLEU-4: 0.866760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTqJb7c5EvhR",
        "outputId": "ea375d7d-dc3e-412a-e2fd-407b60c47d46"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 100:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Czech-both.pkl')\n",
        "train = load_clean_sentences('english-Czech-train.pkl')\n",
        "test = load_clean_sentences('english-Czech-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[ty knihy zde jsou moje], target=[the books here are mine], predicted=[the books here are mine]\n",
            "src=[kdo te naucil psat], target=[who taught you to write], predicted=[who taught you to write]\n",
            "src=[pustte ty psy], target=[release the dogs], predicted=[release the dogs]\n",
            "src=[nas pes kousne zridka], target=[our dog seldom bites], predicted=[our dog seldom bites]\n",
            "src=[dosly nam moznosti volby], target=[were out of options], predicted=[were out of options]\n",
            "src=[nech si ten papir], target=[keep the paper], predicted=[keep the paper]\n",
            "src=[hrajete squash], target=[do you play squash], predicted=[do you play squash]\n",
            "src=[privedl jsem pritele], target=[i brought a friend], predicted=[i brought a friend]\n",
            "src=[prelozili text], target=[they translated the text], predicted=[they translated the text]\n",
            "src=[proc to tom udelal], target=[what made tom do it], predicted=[what made tom do it]\n",
            "src=[vezmu si je s sebou], target=[ill take these with me], predicted=[ill take these with me]\n",
            "src=[nech me abych to nesl], target=[let me carry it], predicted=[let me carry it]\n",
            "src=[porad jeste jis], target=[are you still eating], predicted=[are you still eating]\n",
            "src=[mam to rada], target=[i like it], predicted=[i like it]\n",
            "src=[rada te vidim], target=[nice to see you], predicted=[nice to see you]\n",
            "src=[citil jsem to same], target=[i felt the same way], predicted=[i felt the same way]\n",
            "src=[tom rozevrel pest], target=[tom unclenched his fist], predicted=[tom unclenched his fist]\n",
            "src=[nemam to], target=[i dont have it], predicted=[i dont have it]\n",
            "src=[vis co to je], target=[do you know what this is], predicted=[do you know what this is]\n",
            "src=[je to v lednici], target=[its in the fridge], predicted=[its in the fridge]\n",
            "src=[ted je to dokonale], target=[now its perfect], predicted=[now its perfect]\n",
            "src=[slunce roztalo snih], target=[the sun melted the snow], predicted=[the sun melted the snow]\n",
            "src=[vyhral jsem jackpot], target=[ive hit the jackpot], predicted=[ive hit the jackpot]\n",
            "src=[jeste jste ve skole], target=[are you still in school], predicted=[are you still in school]\n",
            "src=[stravil jsem pondelek s tomem], target=[i spent monday with tom], predicted=[i spent monday with tom]\n",
            "src=[presvedcime toma], target=[well convince tom], predicted=[well convince tom]\n",
            "src=[tom se boji psu], target=[tom is afraid of dogs], predicted=[tom is afraid of dogs]\n",
            "src=[tom nekdy nosi klobouk], target=[tom sometimes wears a hat], predicted=[tom sometimes wears a hat]\n",
            "src=[psi byli celi stastni], target=[the dogs were happy], predicted=[the dogs were happy]\n",
            "src=[zitra je den matek], target=[tomorrow is mothers day], predicted=[tomorrow is mothers day]\n",
            "src=[dosel mi inkoust], target=[i ran out of ink], predicted=[i ran out of ink]\n",
            "src=[nasel jsem tvoje klice], target=[i found your keys], predicted=[i found your keys]\n",
            "src=[radsi to nech tak], target=[youd better leave it], predicted=[youd better leave it]\n",
            "src=[jak dlouho ziji zaby], target=[how long do frogs live], predicted=[how long do frogs live]\n",
            "src=[ochodil jsem si boty], target=[ive worn out my shoes], predicted=[ive worn out my shoes]\n",
            "src=[rad pracuji], target=[i like to work], predicted=[i like to work]\n",
            "src=[kvuli tobe nas vystehovali], target=[you got us evicted], predicted=[you got us evicted]\n",
            "src=[chtel jsem odjet do ciziny], target=[i wanted to go abroad], predicted=[i wanted to go abroad]\n",
            "src=[skoro nic jsem nesnedl], target=[ive eaten almost nothing], predicted=[ive eaten almost nothing]\n",
            "src=[on ma vlastni dum], target=[he has a house of his own], predicted=[he has a house of his own]\n",
            "src=[tom nemel co rict], target=[tom had nothing to say], predicted=[tom had nothing to say]\n",
            "src=[pojdme obchodovat], target=[lets do business], predicted=[lets do business]\n",
            "src=[snez to], target=[eat it], predicted=[eat it]\n",
            "src=[plati me denne], target=[im paid by the day], predicted=[im paid by the day]\n",
            "src=[myslim ze tom je nesmely], target=[i think tom is shy], predicted=[i think tom is shy]\n",
            "src=[udelal jsem to bez rozmyslu], target=[i did it without thinking], predicted=[i did it without thinking]\n",
            "src=[dovolte abych se vam podival na jazyk], target=[let me see your tongue], predicted=[let me see your tongue]\n",
            "src=[jeste neco], target=[anything else], predicted=[anything else]\n",
            "src=[tom je tvuj nepritel], target=[tom is your enemy], predicted=[tom is your enemy]\n",
            "src=[smrdi to tady], target=[it stinks in here], predicted=[it stinks in here]\n",
            "src=[nemaji dukaz], target=[they dont have proof], predicted=[they dont have proof]\n",
            "src=[boli me vzadu na krku], target=[the back of my neck hurts], predicted=[the back of my neck hurts]\n",
            "src=[jsme bezbranni], target=[were defenseless], predicted=[were defenseless]\n",
            "src=[jsi opravdu zvlastni], target=[youre really weird], predicted=[youre really weird]\n",
            "src=[uz tady nemuzu zustat], target=[i cant stay here anymore], predicted=[i cant stay here anymore]\n",
            "src=[tohle je dezinformace], target=[this is misinformation], predicted=[this is misinformation]\n",
            "src=[proste zustan tady], target=[just stay here], predicted=[just stay here]\n",
            "src=[ptaci letaji], target=[birds fly], predicted=[birds fly]\n",
            "src=[na nic nesahej], target=[hey dont touch anything], predicted=[hey dont touch anything]\n",
            "src=[ci je tato taska], target=[whose is this bag], predicted=[whose is this bag]\n",
            "src=[jsem tady rad], target=[i like being here], predicted=[i like being here]\n",
            "src=[tom nas muze zastavit], target=[tom might stop us], predicted=[tom might stop us]\n",
            "src=[delas to spatne], target=[youre doing that wrong], predicted=[youre doing that wrong]\n",
            "src=[jste velice statecna], target=[youre very brave], predicted=[youre very brave]\n",
            "src=[jak to jde], target=[how is it going], predicted=[how is it going]\n",
            "src=[tom se prestal holit], target=[tom stopped shaving], predicted=[tom stopped shaving]\n",
            "src=[neni tady nic], target=[there is nothing here], predicted=[there is nothing here]\n",
            "src=[tom je neoblomny], target=[tom is inflexible], predicted=[tom is inflexible]\n",
            "src=[koho jsi cekal], target=[who did you expect], predicted=[who did you expect]\n",
            "src=[nevypadas prilis dobre], target=[you dont look so hot], predicted=[you dont look so hot]\n",
            "src=[ziji ted v bostonu], target=[i live in boston now], predicted=[i live in boston now]\n",
            "src=[dekuju mnohokrat], target=[thank you very much], predicted=[thank you very much]\n",
            "src=[to rekli oni], target=[they said that], predicted=[they said that]\n",
            "src=[co tim myslis], target=[whats your point], predicted=[whats your point]\n",
            "src=[vim proc me propustili], target=[i know why i was fired], predicted=[i know why i was fired]\n",
            "src=[opravdu musim jit], target=[i really must be going], predicted=[i really must be going]\n",
            "src=[tentokrat mate stesti], target=[this time you are lucky], predicted=[this time you are lucky]\n",
            "src=[nudime se], target=[were bored], predicted=[were bored]\n",
            "src=[restaurace byla ticha], target=[the restaurant was quiet], predicted=[the restaurant was quiet]\n",
            "src=[bojis sa], target=[are you afraid], predicted=[are you afraid]\n",
            "src=[adoptoval sirotka], target=[he adopted the orphan], predicted=[he adopted the orphan]\n",
            "src=[musite si toma udobrit], target=[you need to placate tom], predicted=[you need to placate tom]\n",
            "src=[prevlekni se], target=[get changed], predicted=[change your clothes]\n",
            "src=[dneska si pripadam zabavny], target=[i feel funny today], predicted=[i feel funny today]\n",
            "src=[tady nemuzes plavat], target=[you cant swim here], predicted=[you cant swim here]\n",
            "src=[na tohle bych si mohl zvyknout], target=[i could get used to this], predicted=[i could get used to this]\n",
            "src=[caj je horky], target=[the tea is hot], predicted=[the tea is hot]\n",
            "src=[projevila sve talenty], target=[she displayed her talents], predicted=[she displayed her talents]\n",
            "src=[udelam to sam], target=[ill make it on my own], predicted=[ill make it on my own]\n",
            "src=[tys kdysi ucil ze], target=[you used to teach right], predicted=[you used to teach right]\n",
            "src=[tom mary nedovolil aby to udelala], target=[tom didnt let mary do it], predicted=[tom didnt let mary do it]\n",
            "src=[bezte toma pozdravit], target=[go say hello to tom], predicted=[go say hi to tom]\n",
            "src=[jednou jsem ho videl], target=[i have seen him once], predicted=[i have seen him once]\n",
            "src=[tom vyhral losovani], target=[tom won the toss], predicted=[tom won the toss]\n",
            "src=[tady to stiskni], target=[press here], predicted=[press here]\n",
            "src=[neco potrebuji], target=[i need something], predicted=[i need something]\n",
            "src=[kazdy den jsem chodival na prochazky], target=[i used to walk every day], predicted=[i used to walk every day]\n",
            "src=[ucinek byl bezprostredni], target=[the effect was immediate], predicted=[the effect was immediate]\n",
            "src=[tom ma rad rybu], target=[tom likes fish], predicted=[tom likes fish]\n",
            "src=[dovolte mi abych vam pomohl], target=[let me help you], predicted=[let me help help]\n",
            "BLEU-1: 0.993435\n",
            "BLEU-2: 0.990843\n",
            "BLEU-3: 0.985334\n",
            "BLEU-4: 0.945630\n",
            "test\n",
            "src=[tom se nevzdal], target=[tom didnt give up], predicted=[tom didnt give up]\n",
            "src=[zdavalo se mi o tobe], target=[i used to dream about you], predicted=[i used to dream about you]\n",
            "src=[nespi], target=[he doesnt sleep], predicted=[he doesnt sleep]\n",
            "src=[tom se rychle osprchoval], target=[tom took a quick shower], predicted=[tom took a quick shower]\n",
            "src=[tom mi opravil esej], target=[tom corrected my essay], predicted=[tom corrected my essay]\n",
            "src=[bylo to velmi dobre], target=[that was very good], predicted=[that was very good]\n",
            "src=[kde je ta brana], target=[where is the gate], predicted=[where is the gate]\n",
            "src=[hodne te obdivuji], target=[i admire you a lot], predicted=[i admire you a lot]\n",
            "src=[potrebuju abys poslouchal], target=[i need you to listen], predicted=[i need you to listen]\n",
            "src=[tom mi jeden poslal], target=[tom sent one to me], predicted=[tom sent one to me]\n",
            "src=[nemam v umyslu se vzdat], target=[i dont plan to give up], predicted=[i dont plan to give up]\n",
            "src=[cekaji na nas], target=[theyre waiting for us], predicted=[theyre waiting for us]\n",
            "src=[deti prichazi], target=[here come the children], predicted=[here come the children]\n",
            "src=[mam rad holky], target=[i like girls], predicted=[i like girls]\n",
            "src=[tamty dvere jsem neodemkl], target=[i didnt unlock that door], predicted=[i didnt unlock that door]\n",
            "src=[tenhle pocitac je pomaly], target=[this computer is slow], predicted=[this computer is slow]\n",
            "src=[mam rad fazole], target=[i like beans], predicted=[i like beans]\n",
            "src=[vi jak se citis], target=[does he know how you feel], predicted=[does he know how you feel]\n",
            "src=[tohle dopis], target=[finish this], predicted=[finish this]\n",
            "src=[je tom liny], target=[is tom lazy], predicted=[is tom lazy]\n",
            "src=[tohle pero je nejlepsi], target=[this pen is the best], predicted=[this pen is the best]\n",
            "src=[byval jsem tlusty], target=[i used to be fat], predicted=[i used to be fat]\n",
            "src=[oni nas nenavidi], target=[they hate us], predicted=[they hate us]\n",
            "src=[projizdim], target=[im getting by], predicted=[im getting by]\n",
            "src=[nemam rad pondelky], target=[i dont like mondays], predicted=[i dont like mondays]\n",
            "src=[libi se mi tu], target=[i like being here], predicted=[i like being here]\n",
            "src=[zadne reseni neexistuje], target=[there is no solution], predicted=[there is no solution]\n",
            "src=[tom uz se neboji], target=[tom is no longer afraid], predicted=[tom is no longer afraid]\n",
            "src=[zas jsi to udelal], target=[you did it again], predicted=[you did it again]\n",
            "src=[chce se mi plakat], target=[i feel like crying], predicted=[i feel like crying]\n",
            "src=[byval jsem vrchnim], target=[i used to be a waiter], predicted=[i used to be a waiter]\n",
            "src=[bojovali za svobodu], target=[they fought for freedom], predicted=[they fought for freedom]\n",
            "src=[jsem tlusta], target=[am i fat], predicted=[im i]\n",
            "src=[co vis], target=[what do you know], predicted=[what do you know]\n",
            "src=[chci odejit], target=[i want to quit], predicted=[i want to quit]\n",
            "src=[postaram se o tebe], target=[ill take care of you], predicted=[ill take care of you]\n",
            "src=[byl cely modry zimou], target=[he was blue from the cold], predicted=[he was blue from the cold]\n",
            "src=[jake je jeho skutecne jmeno], target=[whats his real name], predicted=[whats his real name]\n",
            "src=[tom se o tom nikdy nezminil], target=[tom never mentioned that], predicted=[tom never mentioned that]\n",
            "src=[toto ukonci], target=[finish this], predicted=[finish this]\n",
            "src=[stojis mi v ceste], target=[you are in my way], predicted=[you are in my way]\n",
            "src=[ozvi se mi pozdeji prosim], target=[please contact me later], predicted=[please contact me later]\n",
            "src=[pockej v cekarne], target=[wait in the waiting room], predicted=[wait in the waiting room]\n",
            "src=[radsi bych se toho nedotykal], target=[id rather not touch it], predicted=[id rather not touch it]\n",
            "src=[pockam tam na tebe], target=[ill wait there for you], predicted=[ill wait there for you]\n",
            "src=[mame vezne], target=[we have a prisoner], predicted=[we have a prisoner]\n",
            "src=[tom ukazal na marii], target=[tom pointed at mary], predicted=[tom pointed at mary]\n",
            "src=[vcera jsem byl zaneprazdnen], target=[i was busy yesterday], predicted=[i was busy yesterday]\n",
            "src=[tom je ted ve vezeni], target=[tom is now in prison], predicted=[tom is now in prison]\n",
            "src=[mate psa], target=[do you have a dog], predicted=[do you have a dog]\n",
            "src=[co kdybychom se zeptali toma], target=[why dont we ask tom], predicted=[why dont we ask tom]\n",
            "src=[nerad brzo vstavam], target=[i hate getting up early], predicted=[i hate getting up early]\n",
            "src=[dej mi koblihu], target=[give me a donut], predicted=[give me a donut]\n",
            "src=[jen tak si cmaram], target=[im just doodling], predicted=[im just doodling]\n",
            "src=[co tu delas], target=[what are you doing there], predicted=[what are you doing there]\n",
            "src=[je to neuplne], target=[this is imperfect], predicted=[this is imperfect]\n",
            "src=[tom shodil kilo], target=[tom lost kilograms], predicted=[tom lost kilograms]\n",
            "src=[lhalas mi], target=[you lied to me], predicted=[you lied to me]\n",
            "src=[mam rada cokoladu], target=[i like chocolate], predicted=[i like chocolate]\n",
            "src=[musim se posadit], target=[i have to sit down], predicted=[i have to sit down]\n",
            "src=[nebude to pro nas snadne], target=[it wont be easy for us], predicted=[it wont be easy for us]\n",
            "src=[je to neco noveho], target=[is that something new], predicted=[is that something new]\n",
            "src=[uvedomoval sis to], target=[were you aware of this], predicted=[were you aware of this]\n",
            "src=[tom si vycistil zuby], target=[tom brushed his teeth], predicted=[tom brushed his teeth]\n",
            "src=[ztratila jsem svuj destnik], target=[i have lost my umbrella], predicted=[i have lost my umbrella]\n",
            "src=[dobrou noc mami], target=[goodnight mother], predicted=[goodnight mother]\n",
            "src=[jak se jmenujes], target=[what is your name], predicted=[what is your name]\n",
            "src=[myslim ze tom je nevyzraly], target=[i think tom is immature], predicted=[i think tom is immature]\n",
            "src=[tom nevidel nic], target=[tom saw nothing], predicted=[tom saw nothing]\n",
            "src=[koupim tomovi darek], target=[ill buy a gift for tom], predicted=[ill buy a gift for tom]\n",
            "src=[kolik staly ty sklenice], target=[how much were the glasses], predicted=[how much were the glasses]\n",
            "src=[dneska je mi lepe], target=[im better today], predicted=[im better today]\n",
            "src=[prisel jsem na vlak pozde], target=[i was late for the train], predicted=[i was late for the train]\n",
            "src=[ztratila jsem sve oblibene pero], target=[i lost my favorite pen], predicted=[i lost my favorite pen]\n",
            "src=[mas dobrou pamet], target=[you have a good memory], predicted=[you have a good memory]\n",
            "src=[tom neni hloupy kluk], target=[tom isnt a stupid boy], predicted=[tom isnt a stupid boy]\n",
            "src=[mel bys zacit ted], target=[youd better start now], predicted=[youd better start now]\n",
            "src=[odpovedel spatne], target=[he answered incorrectly], predicted=[he answered incorrectly]\n",
            "src=[jsi velmi sympaticky], target=[youre very sympathetic], predicted=[youre very sympathetic]\n",
            "src=[tom omdlel], target=[tom passed out], predicted=[tom passed out]\n",
            "src=[barvim si vlasy], target=[i dye my hair], predicted=[i dye my hair]\n",
            "src=[je zbytecne se toma ptat], target=[its useless to ask tom], predicted=[its useless to ask tom]\n",
            "src=[delas si srandu], target=[are you kidding], predicted=[youre joking kidding]\n",
            "src=[potrebuju papir], target=[i need some paper], predicted=[i need some paper]\n",
            "src=[skutecne neumis plavat], target=[can you really not swim], predicted=[can you really not swim]\n",
            "src=[chodim do kostela kazdy den], target=[i go to church every day], predicted=[i go to church every day]\n",
            "src=[verim ze to tak je], target=[i believe it to be true], predicted=[i believe it to be true]\n",
            "src=[porad chceme vyhrat], target=[we still want to win], predicted=[we still want to win]\n",
            "src=[nebud tak kruty], target=[dont be so cruel], predicted=[dont be so cruel]\n",
            "src=[kdysi tady bydlel tom], target=[tom used to live here], predicted=[tom used to live here]\n",
            "src=[co uci tom], target=[what does tom teach], predicted=[what does tom teach]\n",
            "src=[neni zadna barva], target=[there is no paint], predicted=[there is no paint]\n",
            "src=[mam hodne schopnosti], target=[i have many abilities], predicted=[i have many abilities]\n",
            "src=[rozchodil jsem ten motor], target=[i got the engine going], predicted=[i got the engine going]\n",
            "src=[ta kocka je velka], target=[this cat is big], predicted=[this cat is big]\n",
            "src=[rad te vidim], target=[nice to see you], predicted=[nice to see you]\n",
            "src=[nevinim te ani trochu], target=[i dont blame you one bit], predicted=[i dont blame you one bit]\n",
            "src=[jsem bohaty], target=[im rich], predicted=[im rich]\n",
            "src=[tom nema rad syr], target=[tom doesnt like cheese], predicted=[tom doesnt like cheese]\n",
            "src=[mohu videt svoji dceru], target=[may i see my daughter], predicted=[may i see my daughter]\n",
            "BLEU-1: 0.932431\n",
            "BLEU-2: 0.918535\n",
            "BLEU-3: 0.914338\n",
            "BLEU-4: 0.866760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfxijYxHFC2T",
        "outputId": "b14a3a04-e5fb-4399-e52a-ab04c120c8d3"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-Czech-both.pkl')\n",
        "train = load_clean_sentences('english-Czech-train.pkl')\n",
        "test = load_clean_sentences('english-Czech-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[tom se to nikdy nedozvi], target=[tom will never know], predicted=[tom will never know]\n",
            "src=[moje kolo je cervene], target=[my bike is red], predicted=[my bike is red]\n",
            "src=[to je blbost], target=[thats stupid], predicted=[thats stupid]\n",
            "src=[tom ukazal na marii], target=[tom pointed at mary], predicted=[tom pointed at mary]\n",
            "src=[tady mas nejaky dopis], target=[here is a letter for you], predicted=[here is a letter for you]\n",
            "src=[tom mi stale jeste nezaplatil], target=[tom still hasnt paid me], predicted=[tom still hasnt paid me]\n",
            "src=[doufam ze zustanes cely tyden], target=[i hope you stay all week], predicted=[i hope you stay all week]\n",
            "src=[tom pise pomalu], target=[tom writes slowly], predicted=[tom writes slowly]\n",
            "src=[libi se ti tvoje jmeno], target=[do you like your name], predicted=[do you like your name]\n",
            "src=[podivej se co jsi udelal], target=[look what you did], predicted=[look what you did]\n",
            "BLEU-1: 0.993534\n",
            "BLEU-2: 0.990902\n",
            "BLEU-3: 0.985335\n",
            "BLEU-4: 0.945248\n",
            "test\n",
            "src=[tom se odmita branit], target=[tom refuses to fight back], predicted=[tom refuses to fight back]\n",
            "src=[tom uspel], target=[tom succeeded], predicted=[tom succeeded]\n",
            "src=[tom miluje boston], target=[tom loves boston], predicted=[tom loves boston]\n",
            "src=[nastoupil jsem do spatneho vlaku], target=[i got on the wrong train], predicted=[i got on the wrong train]\n",
            "src=[proc se zdrahas], target=[why do you hesitate], predicted=[why do you hesitate]\n",
            "src=[potrebuje tom asistenci], target=[does tom need assistance], predicted=[does tom need assistance]\n",
            "src=[udelala to podle sebe], target=[she did it in her own way], predicted=[she did it in her own way]\n",
            "src=[ta kniha se mi libila], target=[i liked that book], predicted=[i liked that book]\n",
            "src=[na to zapomen], target=[forget about that], predicted=[forget about that]\n",
            "src=[je to tomova vina], target=[its toms fault], predicted=[its toms fault]\n",
            "BLEU-1: 0.931745\n",
            "BLEU-2: 0.917581\n",
            "BLEU-3: 0.913033\n",
            "BLEU-4: 0.864587\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}